{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "## langsmith Tracking\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACKING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x2d4df9ef230>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://blog.langchain.com/customers-replit/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='\\n\\n\\nPushing LangSmith to new limits with Replit Agent\\'s complex workflows\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPushing LangSmith to new limits with Replit Agent\\'s complex workflows\\nSee how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.\\n\\nCase Studies\\n3 min read\\nSep 26, 2024\\n\\n\\n\\n\\n\\nReplit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow which enables a highly custom agentic workflow with a high-degree of control and parallel execution. By using LangSmith, Replit gained deep visibility into their agent interactions to debug tricky issues.\\xa0The level of complexity required for Replit Agent also pushed the boundaries of LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs. Specifically, there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved performance and scale on large tracesMost other LLMOps solutions monitor individual API requests to LLM providers, offering a limited view of single LLM calls. In contrast, LangSmith from day one has focused on tracing the entire execution flow of an LLM application to provide a more holistic context.\\xa0Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running code, etc). This gives you granular visibility into what’s happening, including at the inputs and outputs of each step, in order to understand the agent’s decision-making.\\xa0Replit Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions – including planning, creating dev environments, installing dependencies, and deploying applications for users.\\xa0As a result, Replit’s traces were very large - involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the LangChain team improved their ingestion to efficiently process and store large volumes of trace data. They also improved LangSmith’s frontend rendering to display long-running agent traces seamlessly.Search and filter within traces to pinpoint issuesLangSmith has always supported search between traces, which allows users to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent’s traces got longer and longer, the Replit team needed to search within traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new search pattern – searching within traces – was added to LangSmith. Instead of sifting and scrolling call-by-call within a large trace, users could now filter directly on a criteria they cared about (e.g. keywords in the inputs or outputs of a run). This greatly reduced Replit’s time needed to debug agent steps within a trace.Thread view to enable human-in-the-loop workflowsA key differentiator of Replit Agent was its emphasis on human-in-the-loop workflows. Replit Agent intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like managing, editing, and verifying generated code,\\xa0 Replit’s agents interacted with users continuously - often over long periods with multiple turns of conversation. However, monitoring these conversational flows was often difficult, as each user session would generate disjoint traces.\\xa0To solve this, LangSmith’s thread view helped collate traces from multiple threads together that were related (i.e. from one conversation). This provided a logical view of all agent-user interactions across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention could be beneficial.\\xa0ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith’s powerful observability features. By reducing the effort of loading long, heavy traces, the Replit team has greatly sped up the process of building and scaling complex agents. With faster debugging, improved trace visibility, and better handling of parallel tasks, Replit is setting the standard for AI-driven development.\\t\\n\\n\\nTags\\nCase Studies\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\\n\\n\\nCase Studies\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\\n\\n\\nCase Studies\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow DocentPro Built a Multi-Agent Travel Companion with LangGraph\\n\\n\\nCase Studies\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Trellix cut log parsing time from days to minutes with LangGraph Studio and LangSmith\\n\\n\\nCase Studies\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTAMM AI Assistant: Transforming Government Services in Abu Dhabi with LangChain and LangGraph\"\\n\\n\\nCase Studies\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Harmonic built an investment agent with LangGraph and LangSmith— so VCs can focus on founders\\n\\n\\nCase Studies\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content=\"Pushing LangSmith to new limits with Replit Agent's complex workflows\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPushing LangSmith to new limits with Replit Agent's complex workflows\\nSee how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.\\n\\nCase Studies\\n3 min read\\nSep 26, 2024\"),\n",
       " Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='Replit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow which enables a highly custom agentic workflow with a high-degree of control and parallel execution. By using LangSmith, Replit gained deep visibility into their agent interactions to debug tricky issues.\\xa0The level of complexity required for Replit Agent also pushed the boundaries of LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs. Specifically, there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved'),\n",
       " Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved performance and scale on large tracesMost other LLMOps solutions monitor individual API requests to LLM providers, offering a limited view of single LLM calls. In contrast, LangSmith from day one has focused on tracing the entire execution flow of an LLM application to provide a more holistic context.\\xa0Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running code, etc). This gives you granular visibility into what’s happening, including at the inputs and outputs of each step, in order to understand the agent’s decision-making.\\xa0Replit Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions – including planning,'),\n",
       " Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions – including planning, creating dev environments, installing dependencies, and deploying applications for users.\\xa0As a result, Replit’s traces were very large - involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the LangChain team improved their ingestion to efficiently process and store large volumes of trace data. They also improved LangSmith’s frontend rendering to display long-running agent traces seamlessly.Search and filter within traces to pinpoint issuesLangSmith has always supported search between traces, which allows users to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent’s traces got longer and longer, the Replit team needed to search within traces for specific'),\n",
       " Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent’s traces got longer and longer, the Replit team needed to search within traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new search pattern – searching within traces – was added to LangSmith. Instead of sifting and scrolling call-by-call within a large trace, users could now filter directly on a criteria they cared about (e.g. keywords in the inputs or outputs of a run). This greatly reduced Replit’s time needed to debug agent steps within a trace.Thread view to enable human-in-the-loop workflowsA key differentiator of Replit Agent was its emphasis on human-in-the-loop workflows. Replit Agent intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like'),\n",
       " Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like managing, editing, and verifying generated code,\\xa0 Replit’s agents interacted with users continuously - often over long periods with multiple turns of conversation. However, monitoring these conversational flows was often difficult, as each user session would generate disjoint traces.\\xa0To solve this, LangSmith’s thread view helped collate traces from multiple threads together that were related (i.e. from one conversation). This provided a logical view of all agent-user interactions across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention could be beneficial.\\xa0ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith’s powerful observability features. By reducing the effort of loading'),\n",
       " Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='where human intervention could be beneficial.\\xa0ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith’s powerful observability features. By reducing the effort of loading long, heavy traces, the Replit team has greatly sped up the process of building and scaling complex agents. With faster debugging, improved trace visibility, and better handling of parallel tasks, Replit is setting the standard for AI-driven development.'),\n",
       " Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='Tags\\nCase Studies\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\\n\\n\\nCase Studies\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\\n\\n\\nCase Studies\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow DocentPro Built a Multi-Agent Travel Companion with LangGraph\\n\\n\\nCase Studies\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Trellix cut log parsing time from days to minutes with LangGraph Studio and LangSmith\\n\\n\\nCase Studies\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTAMM AI Assistant: Transforming Government Services in Abu Dhabi with LangChain and LangGraph\"\\n\\n\\nCase Studies\\n4 min read'),\n",
       " Document(metadata={'source': 'https://blog.langchain.com/customers-replit/', 'title': \"Pushing LangSmith to new limits with Replit Agent's complex workflows\", 'description': 'See how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.', 'language': 'en'}, page_content='Case Studies\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTAMM AI Assistant: Transforming Government Services in Abu Dhabi with LangChain and LangGraph\"\\n\\n\\nCase Studies\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Harmonic built an investment agent with LangGraph and LangSmith— so VCs can focus on founders\\n\\n\\nCase Studies\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb = FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2d4efd15eb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pushing LangSmith to new limits with Replit Agent's complex workflows\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPushing LangSmith to new limits with Replit Agent's complex workflows\\nSee how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.\\n\\nCase Studies\\n3 min read\\nSep 26, 2024\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"LangSmith to new limits with Replit Agent's\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Chain, Documents Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n\\n    Question: {input}\\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002D4EEEE9FA0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002D4EFD16F60>, root_client=<openai.OpenAI object at 0x000002D4EEEEAED0>, root_async_client=<openai.AsyncOpenAI object at 0x000002D4DFC80740>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain is a software framework designed to facilitate the creation of applications using large language models (LLMs). It offers tools and abstractions to streamline the development of applications that leverage the capabilities of LLMs for tasks such as natural language processing, text generation, and more. The framework is commonly used to integrate these models with various data sources and user interactions, allowing developers to build complex and efficient language-based applications more easily.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\": \"explain about langchain?\",\n",
    "    \"context\": [Document(page_content=\"Langchain\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2d4efd15eb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002D4EFD15EB0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n\\n    Question: {input}\\n    '), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002D4EEEE9FA0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002D4EFD16F60>, root_client=<openai.OpenAI object at 0x000002D4EEEEAED0>, root_async_client=<openai.AsyncOpenAI object at 0x000002D4DFC80740>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context does not provide a detailed definition of LangChain. However, it suggests that LangChain is likely a company or platform involved in creating or supporting agent workflows and artificial intelligence systems, as it is mentioned in connection with products like LangGraph and LangSmith, which are used to innovate and improve agentic workflows and AI platforms. Additionally, LangChain seems to be active in organizing and sharing case studies on various applications of their technology.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retriever_chain.invoke({\"input\":\"What is Langchain?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
